{"cells":[{"cell_type":"markdown","metadata":{"id":"e4Gdfuu8h_y3"},"source":["# Import Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51130,"status":"ok","timestamp":1725448097775,"user":{"displayName":"LAURA POLLACCI","userId":"16888075574530779678"},"user_tz":-120},"id":"6inTM2i8BBJN","outputId":"2561dc92-874c-4b29-ff8b-8ee58d90105d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting emojis\n","  Downloading emojis-0.7.0-py3-none-any.whl.metadata (3.5 kB)\n","Downloading emojis-0.7.0-py3-none-any.whl (28 kB)\n","Installing collected packages: emojis\n","Successfully installed emojis-0.7.0\n","Collecting readability\n","  Downloading readability-0.3.1.tar.gz (34 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: readability\n","  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for readability: filename=readability-0.3.1-py3-none-any.whl size=35461 sha256=5808b5a4d986b88c567ad25223285d9d5b587ac014b636c16ce37a0563f48253\n","  Stored in directory: /root/.cache/pip/wheels/05/07/4d/2e3a0aaba1713619a403e1a3c56e88a6fc12d753872b98771c\n","Successfully built readability\n","Installing collected packages: readability\n","Successfully installed readability-0.3.1\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting googletrans\n","  Downloading googletrans-3.0.0.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Collecting httpx==0.13.3 (from googletrans)\n","  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.7.4)\n","Collecting hstspreload (from httpx==0.13.3->googletrans)\n","  Downloading hstspreload-2024.9.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n","Collecting idna==2.* (from httpx==0.13.3->googletrans)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n","  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n","Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n","Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Downloading hstspreload-2024.9.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: langdetect, googletrans\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=e3d7cd3a556cc063ab1527541ead071e9d1fa7a8b0cb90f51208e18d3cc27831\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15718 sha256=79982501fca61697d3b1f3a36cc37da75b327c71eadc999f9566e48f8c07c540\n","  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n","Successfully built langdetect googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, langdetect, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.8\n","    Uninstalling idna-3.8:\n","      Successfully uninstalled idna-3.8\n","Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 langdetect-1.0.9 rfc3986-1.5.0\n","Collecting deep_translator\n","  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (4.12.3)\n","Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (2.32.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.7.4)\n","Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deep_translator\n","Successfully installed deep_translator-1.11.4\n","Collecting language_tool_python\n","  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (24.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.66.5)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (0.44.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2024.7.4)\n","Downloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n","Installing collected packages: language_tool_python\n","Successfully installed language_tool_python-2.8.1\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n","Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n","Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n","Collecting NRCLex\n","  Downloading NRCLex-4.0-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from NRCLex) (0.17.1)\n","INFO: pip is looking at multiple versions of nrclex to determine which version is compatible with other requirements. This could take a while.\n","  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob->NRCLex) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (4.66.5)\n","Building wheels for collected packages: NRCLex\n","  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43309 sha256=70618e5f20c3f58cb52b08d10c30887371e8830ff83b9095c1bc6e830e4f840b\n","  Stored in directory: /root/.cache/pip/wheels/d2/10/44/6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\n","Successfully built NRCLex\n","Installing collected packages: NRCLex\n","Successfully installed NRCLex-3.0.0\n"]}],"source":["!pip install emojis\n","!pip install readability\n","!pip install tqdm\n","!pip install scikit-learn\n","!pip install langdetect googletrans\n","!pip install deep_translator\n","!pip install language_tool_python\n","!pip install contractions\n","!pip install NRCLex"]},{"cell_type":"markdown","metadata":{"id":"uWmor_W3lwih"},"source":["# Code\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23525,"status":"ok","timestamp":1725448121293,"user":{"displayName":"LAURA POLLACCI","userId":"16888075574530779678"},"user_tz":-120},"id":"cy2c26B3ng36","outputId":"25bf257b-5398-4b97-fdb6-30decc63cf14"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package omw-1.4 to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package omw-1.4 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet2021 to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package wordnet2021 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet31 to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package wordnet31 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /Users/gaiasitri/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]}],"source":["# Importazioni\n","import sys\n","import csv\n","import json\n","import math\n","import re\n","import string\n","import time\n","import uuid\n","from datetime import time as dt_time\n","import uuid\n","from langdetect import detect\n","from googletrans import Translator\n","import pandas as pd\n","from nrclex import NRCLex\n","from html import unescape\n","import contractions\n","from textblob import TextBlob\n","import spacy\n","from collections import Counter, defaultdict\n","from itertools import chain\n","from tqdm import tqdm\n","import emojis\n","import language_tool_python\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import readability\n","from math import log\n","\n","\n","nltk.download(\"popular\")\n","stop_words = stopwords.words('english')\n","csv.field_size_limit(sys.maxsize)\n","\n","\n","# Load the spaCy English model\n","nlp = spacy.load('en_core_web_sm')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufg8V5Abt5MW"},"outputs":[],"source":["## FUNZIONI\n","\n","# data nel formato anno-mese.\n","def extract_date_file(stringa):\n","    _, _, annomese = stringa.split('.')[0].rpartition('-')\n","    return annomese\n","\n","# gestione emoji\n","def clean_demojize(text):\n","    demojized_text = emojis.decode(text)\n","    cleaned_text = re.sub(r':([\\w;<>!?/.]+):', r'\\1', demojized_text)\n","    emo_list = emojis.get(text)\n","    emo_list_decoded = [emojis.decode(el) for el in emo_list]\n","    emo_types = emojis.count(text, unique=True)\n","    return cleaned_text, emo_list_decoded, emo_types\n","\n","# Funzione per parolacce e relativi tag\n","def leggi_file_tab():\n","    dizionario = {}\n","    with open(file_parolacce, 'r') as file_uno:\n","        for riga in file_uno:\n","            valori = riga.strip().split('\\t')\n","            dizionario[valori[0]] = valori[1]\n","    return dizionario\n","\n","# Funzione per la polarità\n","def get_sentiment(text):\n","\n","  blob = TextBlob(text)\n","  return blob.sentiment.polarity, blob.sentiment.subjectivity\n","\n","# Funzione per estrarre il conteggio di entità predefinite\n","def count_named_entities(the_doc, labels):\n","  entities = defaultdict(int, {key: 0 for key in labels})\n","\n","  for ent in the_doc.ents:\n","      if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"LOC\"}:\n","          entities[ent.label_] += 1\n","  return dict(entities)\n","\n","\n","# Indice di Herdan\n","def calculate_herdan(list_of_tokens):\n","  num_tokens = len(list_of_tokens)\n","  num_types = len(set(list_of_tokens))\n","  try:\n","    herdan_index = log(num_types) / log(num_tokens)\n","  except:\n","    herdan_index = -1.0\n","  return round(herdan_index, 2)\n","\n","# entropia\n","def calculate_entropy(list_of_tokens):\n","    token_counter = Counter(list_of_tokens)\n","    total_tokens = sum(token_counter.values())\n","    entropy = 0\n","    for count in token_counter.values():\n","        probability = count / total_tokens\n","        if probability > 0:\n","            entropy -= probability * math.log2(probability)\n","\n","    return round(entropy, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEQaBJ6zn1ao"},"outputs":[],"source":["file_parolacce = 'lista_parolacce.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1212,"status":"ok","timestamp":1725449982104,"user":{"displayName":"LAURA POLLACCI","userId":"16888075574530779678"},"user_tz":-120},"id":"Rl57J-_ctyZt","outputId":"2a6100ee-1a59-479f-9eb2-732cf91c0f5b"},"outputs":[{"name":"stderr","output_type":"stream","text":["49it [00:01, 38.45it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Tempo di esecuzione: 0.03 minuti\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["start_time = time.time()\n","\n","\n","parolacce_dict = leggi_file_tab()\n","parolacce_labels = list(set(parolacce_dict.values()))\n","parolacce_data_original = {valore: 0 for valore in parolacce_labels}\n","\n","out_path = ''\n","lista_file = ['./File/sample.csv']\n","\n","lista_sub = ['>', \"\\(\", \"<\", \"\\^\",\"~\", \"\\)\", \"\\[\", \"\\]\", \"\\*\", \"-\", \"$\"]\n","pos_of_interest = ['NOUN', 'ADV', 'ADJ', 'VERB']\n","entities_labels = [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"]\n","emotion_labels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n","readability_labels = [\\\n","    'Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex',\n","    'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex', 'characters_per_word',\n","    'syll_per_word', 'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio',\n","    'characters', 'syllables', 'words', 'wordtypes', 'sentences', 'paragraphs',\n","    'long_words', 'complex_words', 'complex_words_dc'\n","]\n","\n","\n","for nome_file in lista_file:\n","\n","\n","  input_file = out_path + nome_file\n","  out_file = out_path + nome_file.split('.')[0] + '_emo.csv'\n","\n","  with open(out_file, 'w') as output_file:\n","    writer = csv.writer(output_file, delimiter=',')\n","\n","    with open(input_file,'r') as infile:\n","\n","      header = [key for key in list(json.loads(infile.readline()).keys()) if key not in ['gildings', 'awardings']]\n","      header.insert(0, 'id')\n","\n","      header.extend(chain(\n","        emotion_labels,\n","        pos_of_interest,\n","        parolacce_labels,\n","        [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"],\n","        readability_labels\n","      ))\n","\n","      header += ['count_tokens', 'lemmatized_text', 'count_lemmatized_tokens',\n","                 'upper_words', 'count_upper_words',\n","                 'lista_emoji', 'count_emoji', 'count_emoji_type',\n","                 'herdan_value', 'entropy_value', 'count_!', 'count_?', 'count_badwords',\n","                 'sentiment_polarity_textblob', 'sentiment_subjectivity_textblob']\n","      writer.writerow(header)\n","\n","      for line in tqdm(infile):\n","            # Carica il commento come un dizionario Python\n","            comment = json.loads(line)\n","            text = comment['text']\n","\n","            # Crea Id univoco\n","            idn = str(uuid.uuid4()) + f'_{extract_date_file(nome_file)}'\n","            #contrazioni\n","            text = contractions.fix(text)\n","\n","            # Decode HTML entities\n","            text = unescape(text)\n","\n","            # Remove URLs\n","            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n","\n","            # polarità e soggettività\n","            sentiment_polarity, sentiment_subjectivity = get_sentiment(text)\n","\n","            #conteggio di \"!\" e \"?\"\n","            conteggio_punti_esclamativi = len(re.findall(\"!\", text))\n","            conteggio_punti_interrogativi = len(re.findall(\"\\?\", text))\n","\n","            #emoji\n","            text, lista_emoji, emo_types = clean_demojize(text)\n","\n","            # Conteggio delle parolacce per categoria\n","            parolacce_data = defaultdict(int, parolacce_data_original)\n","\n","            # dizionario per parole piene\n","            parole_piene = {valore: 0 for valore in pos_of_interest}\n","\n","            conteggio_parolacce = 0\n","            token_counts = 0\n","\n","            doc = nlp(text)\n","            numero_frasi= len(list(doc.sents))\n","\n","            lemmatized_tokens, upper_words, pos_tag = [], [], []\n","\n","            named_entities = count_named_entities(doc, entities_labels)\n","\n","            total_tokens = len(doc)\n","\n","            for token in doc:\n","              if not token.is_stop:\n","                token_counts += 1\n","                categoria = parolacce_dict.get(token.text.lower())\n","\n","                if categoria is not None:\n","                  conteggio_parolacce += 1\n","                  parolacce_data[categoria] = parolacce_data.get(categoria, 0) + 1\n","\n","\n","                if token.is_upper and len(token) > 1:\n","                  upper_words.append(token.text)\n","\n","                if not token.is_punct and not any(char.isdigit() for char in token.text):\n","                  lemmatized_tokens.append(token.lemma_.lower())\n","                  pos_tag.append(token.pos_)\n","\n","                  if parole_piene.get(token.pos_, None) != None:\n","                    parole_piene[token.pos_] += 1\n","\n","            # Herdan\n","            herdan_value = calculate_herdan(lemmatized_tokens)\n","\n","            # Entropy\n","            entropy_value = calculate_entropy(lemmatized_tokens)\n","\n","            lemmatized_text = \" \".join(lemmatized_tokens)\n","\n","\n","            for el in lista_sub:\n","              lemmatized_text = re.sub(el, ' ', lemmatized_text)\n","            # Rimozione menzione utenti (@username)\n","            lemmatized_text = re.sub(r'@\\w+', ' ', lemmatized_text)\n","            # Remove new lines and tabs\n","            lemmatized_text = re.sub(r'[\\n\\t]', ' ', lemmatized_text)\n","            # al posto del \\ metti lo spazio\n","            lemmatized_text = re.sub(r'\\/', ' ', lemmatized_text)\n","            # Riduce spazi multipli\n","            lemmatized_text = re.sub(r'\\s+', ' ', lemmatized_text).strip()\n","\n","            # Emozioni\n","            affect_frequencies_dict = NRCLex(lemmatized_text).raw_emotion_scores\n","\n","            # Estrazione readability grades e sentence info\n","            try:\n","              r = readability.getmeasures(lemmatized_text, lang='en')\n","              readability_data = dict(r['readability grades'].items())| dict(r['sentence info'].items())\n","            except:\n","              readability_data = None\n","\n","            # Output\n","            output_list = [idn]\n","            output_list += [comment[key] for key in comment.keys() if key not in ['gildings', 'awardings']]\n","\n","            emotion_out_list = [affect_frequencies_dict.get(key, None) for key in emotion_labels]\n","            pos_of_interest_counters = [parole_piene.get(key, None) for key in pos_of_interest]\n","            bad_words_out_list = [parolacce_data[key] for key in parolacce_labels]\n","            ner_out_list = [named_entities[key] for key in entities_labels]\n","            if readability_data:\n","              readability_data_values = [readability_data.get(key, None) for key in readability_labels]\n","            else:\n","              readability_data_values = [None] * len(readability_labels)\n","\n","            output_list.extend(chain(emotion_out_list, pos_of_interest_counters, bad_words_out_list, ner_out_list, readability_data_values))\n","\n","            output_list.extend([total_tokens, lemmatized_text, len(lemmatized_tokens),\n","                                  '_'.join(upper_words), len(upper_words),\n","                                  '_'.join(lista_emoji), len(lista_emoji) , emo_types,\n","                                  herdan_value, entropy_value,\n","                                  conteggio_punti_esclamativi, conteggio_punti_interrogativi, conteggio_parolacce,\n","                                  sentiment_polarity, sentiment_subjectivity])\n","\n","            writer.writerow(output_list)\n","\n","end_time = time.time()\n","\n","# Calcola il tempo trascorso\n","elapsed_time = end_time - start_time\n","\n","# Converti il tempo in minuti\n","elapsed_minutes = elapsed_time / 60\n","\n","print(f\"\\nTempo di esecuzione: {elapsed_minutes:.2f} minuti\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["e4Gdfuu8h_y3"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}